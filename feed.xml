

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://hfadeel.github.io//</id>
  <title>Haytham ElFadeel</title>
  <subtitle></subtitle>
  <updated>2021-08-17T00:03:16-07:00</updated>
  <author>
    <name>Haytham ElFadeel</name>
    <uri>https://hfadeel.github.io//</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="https://hfadeel.github.io//feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="https://hfadeel.github.io//"/>
  <generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator>
  <rights> Â© 2021 Haytham ElFadeel </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Decoupled Transformer</title>
    <link href="https://hfadeel.github.io//posts/dc/" rel="alternate" type="text/html" title="Decoupled Transformer" />
    <published>2021-06-01T19:32:00-07:00</published>
  
    <updated>2021-08-16T23:56:30-07:00</updated>
  
    <id>https://hfadeel.github.io//posts/dc/</id>
    <content src="https://hfadeel.github.io//posts/dc/" />
    <author>
      <name>Haytham ElFadeel</name>
    </author>

  
    
  

  
    <summary>
      





      1. Introduction

Transformer models (e.g. BERT, RoBERTa, ELECTRA) have revolutionized the natural language processing space. Since its introduction there have been many new state-of-the-art  results  in  MRC, NLI, NLU and machine translation. Yet Transformer models are very computationally expensive. There are three major factors that make Transformers models (encoders) expensive:


  The size ...
    </summary>
  

  </entry>

  
  <entry>
    <title>Knowledge Distillation Part 1</title>
    <link href="https://hfadeel.github.io//posts/kd1/" rel="alternate" type="text/html" title="Knowledge Distillation Part 1" />
    <published>2021-03-03T19:32:00-07:00</published>
  
    <updated>2021-08-16T23:56:30-07:00</updated>
  
    <id>https://hfadeel.github.io//posts/kd1/</id>
    <content src="https://hfadeel.github.io//posts/kd1/" />
    <author>
      <name>Haytham ElFadeel</name>
    </author>

  
    
  

  
    <summary>
      





      1. Introduction

Knowledge Distillation is the process of transferring knowledge from a model to another model, usually a smaller one.

Supervised Machine learning relies on labels. Yet these labels provide limited signals. For example, in image recognition tasks, the labels are one-hot vectors with the entire probability assigned to the correct label; those labels do not provide any signal (in...
    </summary>
  

  </entry>

  
  <entry>
    <title>ROaD-Electra Robustly Optimized and Distilled Electra 2</title>
    <link href="https://hfadeel.github.io//posts/road2/" rel="alternate" type="text/html" title="ROaD-Electra Robustly Optimized and Distilled Electra 2" />
    <published>2020-12-25T19:32:00-07:00</published>
  
    <updated>2021-08-16T23:56:30-07:00</updated>
  
    <id>https://hfadeel.github.io//posts/road2/</id>
    <content src="https://hfadeel.github.io//posts/road2/" />
    <author>
      <name>Haytham ElFadeel</name>
    </author>

  
    
  

  
    <summary>
      





      TL;DR

  We present a new state-of-the-art base transformer model for machine reading comprehension that has almost 5% higher F1 score compared to ELECTRA-Base and 2% higher F1 score than the previous state-of-the-art base model (DeBERTa). This Model also outperforms BERT-Large.
  We present more experiments on our new multi-task pre-training method and knowledge distillation which led to this ...
    </summary>
  

  </entry>

  
  <entry>
    <title>ROaD-Electra Robustly Optimized and Distilled Electra 1</title>
    <link href="https://hfadeel.github.io//posts/road1/" rel="alternate" type="text/html" title="ROaD-Electra Robustly Optimized and Distilled Electra 1" />
    <published>2020-10-10T19:32:00-07:00</published>
  
    <updated>2021-08-16T23:56:30-07:00</updated>
  
    <id>https://hfadeel.github.io//posts/road1/</id>
    <content src="https://hfadeel.github.io//posts/road1/" />
    <author>
      <name>Haytham ElFadeel</name>
    </author>

  
    
  

  
    <summary>
      





      Build state-of-the-art Transformer models for QnA, NLI and NLU.

TL;DR

  We present a new multi-task pre training method that improves the transformer performance, generalization, and robustness.
  We present a new variation of knowledge distillation that improves the teacher signal.
  A new state-of-the-art results in SQUAD 2.0 MNLI, and QQP. These improvements come without any increase in in...
    </summary>
  

  </entry>

  
  <entry>
    <title>New Models and Old Tricks</title>
    <link href="https://hfadeel.github.io//posts/old_tricks/" rel="alternate" type="text/html" title="New Models and Old Tricks" />
    <published>2020-05-10T19:32:00-07:00</published>
  
    <updated>2021-08-16T23:56:30-07:00</updated>
  
    <id>https://hfadeel.github.io//posts/old_tricks/</id>
    <content src="https://hfadeel.github.io//posts/old_tricks/" />
    <author>
      <name>Haytham ElFadeel</name>
    </author>

  
    
  

  
    <summary>
      





      It seems tricks (e.g. Data Augmentation, Label smoothing, Mixout) are approaching their limits to improve SoTA models on SQUAD 2.0 and NQA
Natural Language Understanding models have been getting better and better, from ELMo, BERT, GPT, to ALBERT, RoBERTa, ELECTRA and DeBERTa. We already suppress human performance in many narrow tasks. The main way those models have been improving are:

  More t...
    </summary>
  

  </entry>

</feed>


