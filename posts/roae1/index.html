<!DOCTYPE html>







<html lang="en" 
  
    mode="light"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    <!-- i18n for `_javascript/utils/timeago.js` -->
    <meta name="day-prompt" content="days ago">
    <meta name="hour-prompt" content="hours ago">
    <meta name="minute-prompt" content="minutes ago">
    <meta name="justnow-prompt" content="just now">

    

    

  

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Enhancing LLM with high quality data, diverse dataset, and factuality" />
<meta name="author" content="Haytham ElFadeel" />
<meta property="og:locale" content="en" />
<meta name="description" content="Can we close the gap to GPT-4 with a smaller model? A SoTA LLM at home powered by high-quality data, diverse dataset and objective, and fine-tuned for factuality by DPO." />
<meta property="og:description" content="Can we close the gap to GPT-4 with a smaller model? A SoTA LLM at home powered by high-quality data, diverse dataset and objective, and fine-tuned for factuality by DPO." />
<link rel="canonical" href="https://hfadeel.github.io//posts/road2/" />
<meta property="og:url" content="https://hfadeel.github.io//posts/road2/" />
<meta property="og:site_name" content="Haytham ElFadeel" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-25T19:32:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Enhancing LLM with high quality data, diverse dataset, and factuality" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://hfadeel.github.io//posts/raoe1/"},"@type":"BlogPosting","url":"https://hfadeel.github.io//posts/roae1/","headline":"Enhancing LLM with high quality data, diverse dataset, and factuality","dateModified":"2024-01-07T23:56:30-07:00","datePublished":"2024-01-07T19:32:00-07:00","author":{"@type":"Person","name":"Haytham ElFadeel"},"description":"Can we close the gap to GPT-4 with a smaller model? SoTA LLM at home. ","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <title>Enhancing LLM with high quality data, diverse dataset, and factuality | Haytham ElFadeel
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Haytham ElFadeel">
<meta name="application-name" content="Haytham ElFadeel">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
<!--  -->

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">


  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">



  <!-- Manific Popup -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">



  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end" lang="en">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Haytham ElFadeel</a>
    </div>
    <div class="site-subtitle font-italic"></div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://www.linkedin.com/in/hfadeel/" aria-label="linkedin"
        
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['hfadeelm','gmail.com'].join('@')" aria-label="email"
        
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<!--<div id="topbar-wrapper" class="row justify-content-center topbar-down">-->
<!--  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">-->
<!--    <span id="breadcrumb">-->

<!--    -->

<!--    -->

<!--      -->

<!--        -->
<!--          <span>-->
<!--            <a href="/">-->
<!--              Home-->
<!--            </a>-->
<!--          </span>-->

<!--        -->

<!--      -->

<!--        -->

<!--      -->

<!--        -->

<!--          -->
<!--            <span>ROaD-Electra Robustly Optimized and Distilled Electra 2</span>-->
<!--          -->

<!--        -->

<!--      -->

<!--    -->

<!--    </span>&lt;!&ndash; endof #breadcrumb &ndash;&gt;-->

<!--    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>-->

<!--    <div id="topbar-title">-->
<!--      -->
<!--Post-->
<!--      -->
<!--    </div>-->

<!--    <i id="search-trigger" class="fas fa-search fa-fw"></i>-->
<!--    <span id="search-wrapper" class="align-items-center">-->
<!--      <i class="fas fa-search fa-fw"></i>-->
<!--      <input class="form-control" id="search-input" type="search"-->
<!--        aria-label="search" autocomplete="off" placeholder="Search...">-->
<!--      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>-->
<!--    </span>-->
<!--    <span id="search-cancel" >Cancel</span>-->
<!--  </div>-->

<!--</div>-->


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->




<!-- images -->



  <!-- add CDN prefix if it exists -->

  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

  

  <!-- add image placehoder to prevent layout reflow -->

  

  

  
    
      
      
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  

  



<!-- Add lang-badge for code snippets -->




<!-- return -->





<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>Enhancing LLM with high quality data, diverse dataset, and factuality</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Haytham ElFadeel
          </span>
          
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->






  on

<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Jan 07, 2024,  7:32 PM -0700"
  >Jan 07, 2024<i class="unloaded">2024-01-07T19:32:00-07:00</i>
</span>

        </div>

        <div>
          <!-- lastmod -->
          
        
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<!--<span class="readtime" data-toggle="tooltip" data-placement="bottom"-->
<!--  title="1165 words">-->
<!--6 min-->
<!---->
<!--     read-->
<!---->
<!--</span>-->


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

<h1 id="tldr"><strong>TL;DR</strong></h1>
<p>
<ul>
  <li>We present a new State-of-The-Art LLM model that reduce the gap to GPT-4.
  <li>We achieve this by leveraging and focusing on high-quality data, diversity of the datasets and objectives, and factual finetuning with DPO.</li>
</ul>
</p>

<h1 id="1-introduction"><strong>1. Introduction</strong></h1>
<p>
  This project (still in-progress but with impressive results so far) aims to explore the limits of existing LLM performance and enhance existing LLM but not by focusing on the scale (i.e. model size, computation, number of tokens used in training), instead by focusing on data quality and training objectives. The key insights / hypothesis for this work are two folds:
  </p>
  <ol>
  
  <li>The quality of the training data plays a critical role in model performance. While this has been known for decades, almost all LLM today (both open source and proprietary) focus on the size. Earlier research like “Textbooks Are All You Need" provided some insight and motivation for this work.
  
  <li>Diversity of the objectives have shown to improve generalization. We think that increasing the diversity of the tasks with focus on quality, more than the size of the training dataset would result in better generalization. Earlier research like “Robustly Optimized and Distilled Training for Natural Language Understanding” provided some insight and motivation for this work.
  </li>
  </ol>
  <p>
  The starting point (i.e. baseline) is going to be the Mixtral 8x7B model. In section 1, we will recap the basics of the model. In section 3 we will describe our pre-training approach, section 4 will cover our fine-tuning approach that focuses on factuality, section 5 will cover our multi-task fine-tuning approach and our final model, finally section 6 will cover the results.
  </p>


<h1><strong>2. Recap the basics of Mistral and similar modern LLMs</strong></h1>
<p>
  To start I will explain the difference between modern LLM (e.g. Mistral and Llama) and the original Transformer models.
  </p>
  <h2><strong>Training Differences</strong></h2>
  
  
  <p>
  Mistral didn’t release much information about how the model was trained. So we will just skip this part. My best guess is Mistral followed Llama recipe but with tuning such as: better pre-processing of the data, better data filtration for diversification, deduplication, bias, and quality, and more recent dataset, etc.
  </p>
  <h2><strong>Architecture Differences</strong></h2>
  
  
  <p>
  Most recent LLM such as Mistral, Llama follow what we call Transformer++ which a a pre–norm transformer + RMS normalization, SwiGLU activation in inverted bottleneck FFN, and rotary embedding. Here are summary of those changes:
  </p>
  <h3><strong>Rotary Embedding</strong></h3>
  
  
  <p>
  Before Rotary Embedding improves upon and unify absolute and relative embedding. Rotary Embedding (i.e. RoPE) encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation.
  </p>
  <p>
  The mechanism is simple, we like a positional encoding function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi>f</mi>
    <mrow>
      <mo stretchy="true" form="prefix">(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mo stretchy="true" form="postfix">)</mo>
    </mrow>
  </mrow>
</math>
   
   for an item <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>x</mi>
  </math>
  
   and its position <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>l</mi>
  </math>
  
   
   such that, for two items <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>q</mi>
  </math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>k</mi>
  </math>
    
   at position <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>m</mi>
  </math>
  
   and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>n</mi>
  </math>
  
  , the inner product between 
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mi>f</mi>
      <mrow>
        <mo stretchy="true" form="prefix">(</mo>
        <mi>q</mi>
        <mo>,</mo>
        <mi>m</mi>
        <mo stretchy="true" form="postfix">)</mo>
      </mrow>
    </mrow>
  </math>
   and 
   <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mi>f</mi>
      <mrow>
        <mo stretchy="true" form="prefix">(</mo>
        <mi>k</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo stretchy="true" form="postfix">)</mo>
      </mrow>
    </mrow>
  </math>
  
   is sensitive only to the values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>q</mi>
  </math>
  , <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>k</mi>
  </math>
  
  , and their relative position. Rotary embedding achieve this by taking into account that the dot product between two vectors is a function of the magnitude of individual vectors and the angle between them. With this in mind, the intuition behind RoPE is that we can represent the token embeddings as complex numbers and their positions as pure rotations that we apply to them.
  </p>
  <p>
  For more details, this <a href="https://blog.eleuther.ai/rotary-embeddings/">article</a> provide in depth overview of Rotary Embedding
  </p>
  <p>
  <a href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L132">Here</a> is Llama code for applying rotary embedding
  </p>


  <h3><strong>SwiGLU Activation function in FFN</strong></h3>


  <p>
  Both LLaMA and Mistral uses SwiGLU, 
  </p>
  <p>
    <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
      <mrow>
        <mi>F</mi>
        <mi>F</mi>
        <mi>N</mi>
        <mi>S</mi>
        <mi>w</mi>
        <mi>i</mi>
        <mi>G</mi>
        <mi>L</mi>
        <mi>U</mi>
        <mrow>
          <mo stretchy="true" form="prefix">(</mo>
          <mi>x</mi>
          <mo>,</mo>
          <mi>w</mi>
          <mn>1</mn>
          <mo>,</mo>
          <mi>w</mi>
          <mn>2</mn>
          <mo>,</mo>
          <mi>w</mi>
          <mn>3</mn>
          <mo stretchy="true" form="postfix">)</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo stretchy="true" form="prefix">(</mo>
          <mi>S</mi>
          <mi>w</mi>
          <mi>i</mi>
          <mi>s</mi>
          <mi>h</mi>
          <mn>1</mn>
          <mrow>
            <mo stretchy="true" form="prefix">(</mo>
            <mi>x</mi>
            <mi>W</mi>
            <mn>1</mn>
            <mo stretchy="true" form="postfix">)</mo>
          </mrow>
          <mi>x</mi>
          <mo stretchy="true" form="postfix">)</mo>
        </mrow>
        <mo>⊗</mo>
        <mi>x</mi>
        <mi>W</mi>
        <mn>2</mn>
        <mo stretchy="false" form="postfix">)</mo>
        <mi>W</mi>
        <mn>3</mn>
      </mrow>
    </math>
    <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
      <mrow>
        <mi>S</mi>
        <mi>w</mi>
        <mi>i</mi>
        <mi>s</mi>
        <mi>h</mi>
        <mi>β</mi>
        <mrow>
          <mo stretchy="true" form="prefix">(</mo>
          <mi>x</mi>
          <mo stretchy="true" form="postfix">)</mo>
        </mrow>
        <mo>=</mo>
        <mi>x</mi>
        <mi>σ</mi>
        <mrow>
          <mo stretchy="true" form="prefix">(</mo>
          <mi>β</mi>
          <mi>x</mi>
          <mo stretchy="true" form="postfix">)</mo>
        </mrow>
      </mrow>
    </math>
  <p>
  SwiGLU was introduced in 2020 [<a href="https://arxiv.org/pdf/2002.05202.pdf">https://arxiv.org/pdf/2002.05202.pdf</a>] and it’s known to consistently outperforms ReLU, GLU, and GELU, equally important the Swish family of functions (e.g. Swish, SwiGLU) are more stable and smoother, which lead to faster conversion, and potentially better generalization.
  </p>
  <p>

    <p>
      <img src="/assets/img/blog/roae1_1.png"/>
      The output landscape of a random neural network with different activation functions. Specifically, we randomly initialize a 6-layered neural network, pass in as input the x and y coordinates of each point in a grid, and plot the scalar network output for each grid point. from the paper.
    </p>
  <h3><strong>FFN with Inverted bottleneck</strong></h3>
  <p>
    Both Mistral and LLAMA have FFN blocks with 3 weight matrices, as opposed to two for the original Transformer. That’s because the SwiGLU contains 3 matrices (W1, W2, W3). This doesn’t change the computation or the total number of parameters meaningfully, as the number of hidden units dff (the second dimension of W1 and W2 and the first dimension of W3) by a factor of ⅔ when comparing these layers to the original two-matrix version.
  </p>
  <h3><strong>RMSNorm</strong></h3>
  <p>
    Instead of Layer normalization, RMS Normalization is a simple modification that gets rid of the re-centering invariance in LayerNorm.
  </p>
  <p>
    <strong>LayerNorm:</strong>
    </p>
    <p>
      <img style="width:150px;" src="/assets/img/blog/roae1_2.png"/>
      <img style="width:300px;display:block;"  src="/assets/img/blog/roae1_3.png"/>
    </p>
    <p>
      <strong>RMSNorm:</strong>
      </p>
      <p>
        <img style="width:400px;" src="/assets/img/blog/roae1_4.png"/>
      </p>
      <h3><strong>Grouped-Query Attention</strong></h3>
      <p>
        Multi-head attention (MHA) while achieved impressive results, it’s very expensive computationally, specially for long context which is typical for modern LLM. multi-query attention (MQA) is a mechanism that uses only a single key-value head for multiple queries, which can save memory and greatly speed up decoder inference. However, MQA can lead to quality degradation. Grouped Query Attention (GQA) [<a href="https://arxiv.org/pdf/2305.13245.pdf">https://arxiv.org/pdf/2305.13245.pdf</a>] is a method that sit between multi-query attention (MQA) and multi-head attention (MHA). It aims to achieve the quality of MHA while maintaining the speed of MQA.
      </p>
      <p>
        Grouped-query attention divides query heads into G groups, each of which shares a single key head and value head. GQA-G refers to grouped-query with G groups. GQA-1, with a single group and therefore single key and value head, is equivalent to MQA, while GQA-H, with groups equal to number of heads, is equivalent to MHA.
      </p>
      <p>
        <img src="/assets/img/blog/roae1_5.png"/>
        <a href="https://github.com/fkodom/grouped-query-attention-pytorch/blob/main/grouped_query_attention_pytorch/attention.py">Here</a> is a simple implementation by the GQA author. <a href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L176">Here</a> is the Llama implementation for GQA.
      </p>

      <h3><strong>Sliding Window Attention (SWA)</strong></h3>
      <p>
        Llama doesn’t employ SAW, only Mistral. The sliding window attention reduces computation and memory need for LLM by making each layer attend to the previous N tokens (e.g. 4,096), instead of all the previous tokens. Given the stacked nature of the Transformer attention architecture, the sliding window still can attend to the tokens that are far beyond the N (e.g. 4,096).  A token 
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>i</mi>
        </math>
 at layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>k</mi>
</math>

 attends to tokens 

 <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mo stretchy="true" form="prefix">[</mo>
    <mi>i</mi>
    <mo>−</mo>
    <mi>S</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>W</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mo>,</mo>
    <mi>i</mi>
    <mo stretchy="true" form="postfix">]</mo>
  </mrow>
</math>

 at layer 

 <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi>k</mi>
    <mo>−</mo>
    <mn>1</mn>
  </mrow>
</math>

. These tokens attended to tokens

<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mo stretchy="true" form="prefix">[</mo>
    <mi>i</mi>
    <mo>−</mo>
    <mn>2</mn>
    <mo>*</mo>
    <mi>S</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>W</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mo>,</mo>
    <mi>i</mi>
    <mo stretchy="true" form="postfix">]</mo>
  </mrow>
</math>

. Higher layers have access to information further in the past than what the attention patterns seem to entail. <a href="https://github.com/mistralai/mistral-src/blob/main/mistral/model.py#L101">Here</a> is the Mistral implementation for SWA.
</p>
<p>
  <img style="display: block;" src="/assets/img/blog/roae1_6.png"/>
  From <a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>
</p>

<h3><strong>Mixture of expert</strong></h3>


<p>
Only Mixtral 8x7B applies Mixture of experts. One way to think about the FFN and the Attention blocks within each of the transformer layers is that, the attention tries to understand what the token means given all other tokens (based on the idea that the meaning of a word depend on the other words in the sentence/context) and collect information about the neighbors or interact with the neighbors, the FFN on the other hands tries to think or process that information. Mixture of expert (i.e. MoE) allows us to have multiple experts based on the token or some gating mechanism.
</p>
<p>
MoE allows us to build sparse models and decouple the model capacity (number of parameters) from the compute requirements, by only activating a subset of weights (experts) based on gating mechanisms. While MoE allows us to increase the model capacity without incurring the computation significantly, it still requires the same amount of memory bandwidth for large batch size. Memory bandwidth is currently the biggest bottleneck for training and inference of LLMs.
</p>
<p>
The Mixtral 8x7b design follows the Switch Transformer [<a href="https://arxiv.org/pdf/2101.03961.pdf">https://arxiv.org/pdf/2101.03961.pdf</a>]. It implements 8 experts in the FFN, and each token is routed to top-k experts.
</p>
<p>


<p id="gdcalert26" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert27">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="/assets/img/blog/roae1_7.png"/>

</p>
<p>
From <a href="https://github.com/mistralai/mistral-src/tree/main?tab=readme-ov-file#sparse-mixture-of-experts-smoe">https://github.com/mistralai/mistral-src/tree/main?tab=readme-ov-file#sparse-mixture-of-experts-smoe</a>
</p>

<h1><strong>3. Pretraining</strong></h1>
<h2><strong>Preliminaries</strong></h2>


<p>
The objective of this project is to explore the limit of LLMs performance starting from known good baseline, with special focus on the role of data quality, objective diversity, factuality, and commonsense reasoning. The key insights / hypothesis for this work are two folds:
</p>
<ol>

<li>The quality of the training data plays a critical role in model performance. While this has been known for decades, almost all LLM today (both open source and proprietary) focus on the size. Earlier research like “Textbooks Are All You Need" provided some insight and motivation for this work.

<li>Diversity of the objectives have shown to improve generalization. We think that increasing the diversity of the tasks with focus on quality, more than the size of training data would result in better generalization. Earlier research like “Robustly Optimized and Distilled Training for Natural Language Understanding” provided some insight and motivation for this work.
</li>
</ol>
<p>
We start by resuming the pretraining process of the baseline model, which is Mixtral 8x7B with high quality data.
</p>
<h2><strong>PreTraining data</strong></h2>


<p>
Our pre-training data is a mixture of synthetically generated data specifically created to teach the model commonsense reasoning and general knowledge, including science, cause and effect, and daily activities, among others and a selected web data that is carefully processed to ensure high quality.
</p>
<h3><strong>Web data</strong></h3>


<p>
The web portion of our pre training dataset are from the following websites:
</p>
<ul>

<li>Wikipedia

<li>Wikiversity

<li>WikiBooks

<li>Wikivoyage

<li>Stack Exchange
</li>
</ul>
<p>
Plus a selected number of web-pages referenced from mentioned websites. A special attention was paid to the quality of the data. The data was cleaned and prepared to ensure model inputs are high quality that includes:
</p>
<ul>

<li>The boundaries between pages was respected so the model doesn’t take input from two unrelated pages in one sample.

<li>Tables, info-boxes, Math LaTeX, and image captions was processed.
</li>
</ul>
<h3><strong>Syntactic data</strong></h3>


<p>
The synthetic dataset was generated from several sources to improve the model understanding of commonsense, world knowledge, and factuality from the following sources:
</p>
<ul>
<li>ATOMIC 2020 [<a href="https://www.semanticscholar.org/paper/COMET-ATOMIC-2020%3A-On-Symbolic-and-Neural-Knowledge-Hwang-Bhagavatula/e39503e01ebb108c6773948a24ca798cd444eb62">paper</a>] [<a href="https://mosaickg.apps.allenai.org/kg_atomic2020">website</a>]

<p>
Common sense dataset contains about 1.07M of everyday inferential knowledge tuples about entities and events.
</p>
</li>
</ul>

<ul>
<li>ConceptNet [<a href="https://arxiv.org/abs/1612.03975">paper</a>] [<a href="https://conceptnet.io/">website</a>]
<p>
Contains about 1.6M tuple of basic common sense knowledge about world entities.
</p>
</li>
</ul>


<ul>
<li>Quasimodo [<a href="https://arxiv.org/abs/1905.10989">paper</a>] [<a href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/commonsense/quasimodo/">website</a>]
<p>
contains about 6.2M tuple of basic common sense knowledge about world entities.
</p>
</li>
</ul>


<ul>
<li>ASCENT [<a href="https://arxiv.org/pdf/2011.00905.pdf">paper</a>] [<a href="https://ascent.mpi-inf.mpg.de/">website</a>]
<p>
    Contains 8.92M tuple of basic common sense knowledge about world entities.
</p>
</li>
</ul>
<p>
We sampled a subset of the tuples, merged them by the subject, and filtered them using heuristics to ensure the educational value, then we converted the tuple into sentences. Examples of tuples that was removed and kept:
</p>
<ul>

<li>Removed: 	&nbsp;PersonX borrows money	oWant	none

<li>Removed: 	&nbsp;PersonX borrows money	xAttr	needy

<li>Kept: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PersonX borrows money	xEffect	becomes in debt
</li>
</ul>
<h2><strong>Training Details</strong></h2>


<p>
The training data was roughly 10B tokens and we trained Mixtral 8x7B on 8 x H100 GPUs for several days. We call this model <strong>‘ROaE base v1 - Mixtral 8x7B</strong>’ (<strong>R</strong>obustly <strong>O</strong>ptimized and <strong>E</strong>nhanced). This model did not undergone alignment through reinforcement learning from human feedback (RLHF), nor has it been instruct fine-tuned.
</p>
<h1><strong>4. Fine-tuning model [v1]</strong></h1>


<p>
Our objective with fine-tuning the model is to improve factuality, reduce hallucination, and show the generalization capabilities of our model. Our approach is largely inspired by Fine-tuning Language Models for Factuality [<a href="https://arxiv.org/pdf/2311.08401.pdf">https://arxiv.org/pdf/2311.08401.pdf</a>]. We’re leveraging two key recent innovations:
</p>
<ol>

<li>The ability to judge the factuality of open-ended text by measuring consistency with an external knowledge base.

<li>Direct Preference Optimization (DPO) algorithm which enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses.
</li>
</ol>
<p>
For our fine-tuning we’re using two datasets:
</p>
<ol>

<li>Publicly available Instruction datasets from HuggingFace.

<li>Syntactic question answering data generated from Wikipedia websites. The questions generated programmatically and aims to cover realistic usage distribution, covers both short and long answers, and question types (e.g. where was personX born, write a bio about personX, summarize articleX, what is the timeline of entityX, Did entityX do eventY), We also randomly inserted adversarial fact in the prompt/question (e.g. perxonX is american, where was personX born, perxonY was born in germany, where was personX born).
</li>
</ol>
<p>
We rank responses based on factuality, for questions where we expected a short answer (single fact) (e.g. where was personX born) we use the correct answer + heuristic to rank the responses, our heuristic rank concise answer higher. For open-ended questions (e.g. summarization, write bio) we using FactScore [<a href="https://arxiv.org/pdf/2305.14251.pdf">https://arxiv.org/pdf/2305.14251.pdf</a>] which leverage GPT-3.5 to extract the atomic claims from the model responses, then we used a small model (e.g. Mistral 7B) along with the source page to estimate the truthfulness [in follow up work, we plan drop the dependencies on external LLMs]. We call this model <strong>‘ROaE DPO v1- Mixtral 8x7B</strong>’.
</p>
<h1><strong>5. Fine-tuning model [v2]</strong></h1>


<p>
Given that we’re trying to push the model performance to the limit and Peter Norvig famous quote “<strong><em>More data beats clever algorithms, but better data beats more data</em></strong>” and knowing that proprietary models like GPT-4, Claude, and others don’t constrain themself with a specific publicly available datasets nor they publish the dataset used for their alignment process. We came up with alternative fine-tuning approach that leverage more objective diversity, and increase the training data size significantly. 
</p>
<p>
<h2><strong>Training data:</strong></h2>
</p>
<p>
We first increase the size and diversity of the syntactic question answering dataset we created from Wikipedia, then we sampled a subset of the following publicly available dataset:
</p>
<ul>
<li>CommonsenseQA [<a href="https://arxiv.org/abs/1811.00937">paper</a>] [<a href="https://www.tau-nlp.org/commonsenseqa">website</a>]
<p>
A Question Answering Challenge Targeting Commonsense Knowledge with about 9K training examples.
</p>
</li>
</ul>

<ul>
<li>SocialIQA [<a href="https://arxiv.org/abs/1904.09728">paper</a>] [<a href="https://leaderboard.allenai.org/socialiqa/submissions/get-started">website</a>]
<p>
    Commonsense Reasoning about Social Interactions with about 33K training example of multi-choice questions
</p>
</li>
</ul>

<ul>
<li>PhysicalIQA [<a href="https://arxiv.org/abs/1911.11641">paper</a>] [<a href="https://leaderboard.allenai.org/physicaliqa/submissions/get-started">website</a>]
<p>
    Dataset Reasoning about Physical Commonsense in Natural Language with about 20K training examples
</p>
</li>
</ul>

<ul>

<li>OpenbookQA [<a href="https://arxiv.org/abs/1809.02789">paper</a>] [<a href="https://leaderboard.allenai.org/open_book_qa/submissions/get-started">website</a>]
<p>

    Dataset for reasoning about commonsense and knowledge with about 5K training examples.
</p>
</li>
</ul>

<ul>

<li>ProtoQA [<a href="https://arxiv.org/abs/2005.00771">paper</a>] [<a href="https://github.com/iesl/protoqa-data">website</a>]
<p>

    A Question Answering Dataset for Prototypical Common-Sense Reasoning with about 8K training examples.
</p>
</li>
</ul>

<ul>

<li>ReCoRD [<a href="https://arxiv.org/abs/1810.12885">paper</a>] [<a href="https://sheng-z.github.io/ReCoRD-explorer">website</a>]
<p>

    Commonsense-based reading comprehension with a focus on news articles with about 100K training examples.
</p>
</li>
</ul>
<p>
To ensure fairness and apple-to-apple we didn’t not use any dataset that is currently widely used to evaluate the performance of LLMs (e.g. ARC, Natural QUestions, TriviaQA, MMLU, HellaSwag, WinoGrande, GSM8K).
</p>
<h2><strong>Training Details</strong></h2>


<p>
We trained a 5 models starting from the base model <strong>‘ROaE base - Mixtral 8x7B</strong>’, each model trained using randomly sampled 90% of the entire fine-tuning dataset, also using different shuffling seed. We then merge the models using Fisher-Weighted Averaging [<a href="https://arxiv.org/abs/2111.09832">https://arxiv.org/abs/2111.09832</a>]. Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. Fisher-Weight averaging have show to work slightly better any regular averaging [Model soup <a href="https://arxiv.org/pdf/2203.05482.pdf">https://arxiv.org/pdf/2203.05482.pdf</a>]. The merged model is our final model which we call <strong>‘ROaE DPO v2 - Mixtral 8x7B</strong>’.
</p>
<h1><strong>6. Results</strong></h1>
<img style="max-width:200%" src="/assets/img/blog/roae1_8.png"/>



      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
<!--        -->

        <!-- tags -->
<!--        -->

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          <div class="license-wrapper">
<!--          -->
<!--            -->
<!--            This post is licensed under -->
<!--            <a href="https://creativecommons.org/licenses/by/4.0/">-->
<!--              CC BY 4.0-->
<!--            </a>-->
<!--             by the author.-->
<!--          -->
          </div>

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hfadeel.github.io//posts/road2/" data-toggle="tooltip" data-placement="top"
          title="LinkedIn" target="_blank" rel="noopener" aria-label="LinkedIn">
          <i class="fa-fw fab fa-linkedin"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')"
        data-toggle="tooltip" data-placement="top"
        title="Copy link">
    </i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->





<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  <!---->

<!---->

<!---->

<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->

<!---->

<!---->

<!---->
<!--  -->
<!---->
<!--  -->
<!---->
<!--  -->
<!---->
<!--  -->
<!---->
<!--  -->
<!---->


  

  

















  
  </div> <!-- .access -->

  
    <!-- BS-toc.js will be loaded at medium priority -->
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
    <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
      <nav id="toc" data-toggle="toc"></nav>
    </div>
  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">




    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center">
    <div class="footer-left">
      <p class="mb-0">
        © 2024
        <a href="">Haytham ElFadeel</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://hfadeel.github.io/{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup -->
  <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>





<script defer src="/assets/js/dist/post.min.js"></script>



<!-- commons -->

<script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>




  </body>

</html>

