<!DOCTYPE html>







<html lang="en" 
  
    mode="light"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    <!-- i18n for `_javascript/utils/timeago.js` -->
    <meta name="day-prompt" content="days ago">
    <meta name="hour-prompt" content="hours ago">
    <meta name="minute-prompt" content="minutes ago">
    <meta name="justnow-prompt" content="just now">

    

    

  

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Decoupled Transformer" />
<meta name="author" content="Haytham ElFadeel" />
<meta property="og:locale" content="en" />
<meta name="description" content="1. Introduction" />
<meta property="og:description" content="1. Introduction" />
<link rel="canonical" href="https://hfadeel.github.io//posts/dc/" />
<meta property="og:url" content="https://hfadeel.github.io//posts/dc/" />
<meta property="og:site_name" content="Haytham ElFadeel" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-01T19:32:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Decoupled Transformer" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://hfadeel.github.io//posts/dc/"},"@type":"BlogPosting","url":"https://hfadeel.github.io//posts/dc/","headline":"Decoupled Transformer","dateModified":"2021-08-16T23:56:30-07:00","datePublished":"2021-06-01T19:32:00-07:00","author":{"@type":"Person","name":"Haytham ElFadeel"},"description":"1. Introduction","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <title>Decoupled Transformer | Haytham ElFadeel
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Haytham ElFadeel">
<meta name="application-name" content="Haytham ElFadeel">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
<!--  -->

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">


  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">



  <!-- Manific Popup -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">



  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end" lang="en">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Haytham ElFadeel</a>
    </div>
    <div class="site-subtitle font-italic"></div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://www.linkedin.com/in/hfadeel/" aria-label="linkedin"
        
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['hfadeelm','gmail.com'].join('@')" aria-label="email"
        
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<!--<div id="topbar-wrapper" class="row justify-content-center topbar-down">-->
<!--  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">-->
<!--    <span id="breadcrumb">-->

<!--    -->

<!--    -->

<!--      -->

<!--        -->
<!--          <span>-->
<!--            <a href="/">-->
<!--              Home-->
<!--            </a>-->
<!--          </span>-->

<!--        -->

<!--      -->

<!--        -->

<!--      -->

<!--        -->

<!--          -->
<!--            <span>Decoupled Transformer</span>-->
<!--          -->

<!--        -->

<!--      -->

<!--    -->

<!--    </span>&lt;!&ndash; endof #breadcrumb &ndash;&gt;-->

<!--    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>-->

<!--    <div id="topbar-title">-->
<!--      -->
<!--Post-->
<!--      -->
<!--    </div>-->

<!--    <i id="search-trigger" class="fas fa-search fa-fw"></i>-->
<!--    <span id="search-wrapper" class="align-items-center">-->
<!--      <i class="fas fa-search fa-fw"></i>-->
<!--      <input class="form-control" id="search-input" type="search"-->
<!--        aria-label="search" autocomplete="off" placeholder="Search...">-->
<!--      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>-->
<!--    </span>-->
<!--    <span id="search-cancel" >Cancel</span>-->
<!--  </div>-->

<!--</div>-->


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->




<!-- images -->



  <!-- add CDN prefix if it exists -->

  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

  

  <!-- add image placehoder to prevent layout reflow -->

  

  

  
    
      
      
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  

  



<!-- Add lang-badge for code snippets -->




<!-- return -->





<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>Decoupled Transformer</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Haytham ElFadeel
          </span>
          
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->






  on

<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Tue, Jun  1, 2021,  7:32 PM -0700"
  >Jun  1<i class="unloaded">2021-06-01T19:32:00-07:00</i>
</span>

        </div>

        <div>
          <!-- lastmod -->
          
          <span>
            Updated
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->






<span class="timeago lastmod"
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Mon, Aug 16, 2021, 11:56 PM -0700"
  >Aug 16<i class="unloaded">2021-08-16T23:56:30-07:00</i>
</span>

          </span>
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<!--<span class="readtime" data-toggle="tooltip" data-placement="bottom"-->
<!--  title="1321 words">-->
<!--7 min-->
<!---->
<!--     read-->
<!---->
<!--</span>-->


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <h2 id="1-introduction"><strong>1. Introduction</strong></h2>

<p>Transformer models (e.g. BERT, RoBERTa, ELECTRA) have revolutionized the natural language processing space. Since its introduction there have been many new state-of-the-art  results  in  MRC, NLI, NLU and machine translation. Yet Transformer models are very computationally expensive. There are three major factors that make Transformers models (encoders) expensive:</p>

<ol>
  <li>The size of the feed-forward layers which expand, activate then compress the representation.</li>
  <li>The attention layer, while transformer avoids the sequential nature of RNNl it’s prohibitively expensive for long sequences because of its quadratic nature.</li>
  <li>The number of layers.</li>
</ol>

<p>There have been many ideas to make Transformers more performant, such as: Precision Reduction (Quantization), Distilling to a smaller architecture and Approximate Attention.</p>

<p>Here, we investigate another approach that is perpendicular to all other approaches (which means it can work alongside them). We call this approach <strong>Decoupled Transformer</strong>. Which decouple the inputs to improve efficiency.</p>

<h2 id="2-decoupled-transformer"><strong>2. Decoupled Transformer:</strong></h2>
<p>The idea of Decoupled Transformer is inspired by two things:</p>

<ol>
  <li>The fact that we can give humans a set of passages, then a question and the human will be able to answer the question from the passages.</li>
  <li>In Transformer we concatenate the input (e.g. question and passage) and run them together through all the layers, But how much cross-attention (attention between the inputs, e.g. question and passage) is really needed?</li>
</ol>

<p>In tasks where part of the transformer inputs doesn’t change often or could be cached, such as: Document Ranking in Information Retrieval (where the documents don’t change often),  Question Answering (aka MRC) (where the passages don’t change often), Natural Language Inference  Similarity matching, etc.</p>

<p>The decoupled transformer aims to reduce the inference efficiency by processing the inputs independently for part of the process and eliminating redundant  computation, then process the inputs jointly for the later part of the proces.</p>

<p>Decoupled transformer splits the transformer model into two components, an Input-Component (the lower N layers) which processes the inputs independently and produce a representation, which is cached and reused; and the Cross-Component (the higher M layers) which processes the inputs jointly (after concatenation) and produces the final output.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/dc1.png" alt="desktopview" /></p>

<h3 id="21--decoupled-transformer-workflow-in-qna"><strong>2.1.  Decoupled Transformer Workflow in QnA</strong></h3>

<p>In information retrieval, question answering or similar use cases, we run the Input-Component part of the model on each document/passage while indexing the corpus and store the representation with the document/passage record (A in the image).</p>

<p>During inference (search or answering) we compute the query/question representation using the Input-Component part of the model as well, then we retrieve the candidate documents/passages with their representation from the index/DB, then we concatenate the query/question representation with the retrieved document/passage representation and run them though the Cross-Component part of the model. Figure 2 shows the workflow of the decoupled transformer in information retrieval/question answering scenario (B in the image).</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/dc2.png" alt="desktopview" /></p>

<h3 id="22-benefits-and-cost"><strong>2.2. Benefits and Cost</strong></h3>

<p><strong>Benefits:</strong></p>

<ul>
  <li>Reduce the effective inference computation by caching and reusing the presentation of the long expensive part of the input, which is the document/passage - which is significantly longer than the query/question.</li>
  <li>Eliminate the redundant query/question representation computation which happens in the typical method of processing.</li>
</ul>

<p><strong>Cost:</strong></p>

<ul>
  <li>Decoupled Transformer requires storing  the document/passage representation which could be significant, but we will discuss ways to reduce it.</li>
</ul>

<h2 id="3-experiments"><strong>3. Experiments</strong></h2>

<h3 id="31-creating-decoupled-transformer"><strong>3.1 Creating Decoupled Transformer</strong></h3>

<p>We start from a fine-tuned model, then we create the decoupled model by splitting the encoder layers into Input-Component and Cross-Component, note we keep the learned weight unchanged. We also create a position embedding and segment embedding layers at the start of the Cross-Component and initialize them to the same weights as the position and segment embedding from the decoupled part.</p>

<p>Starting from a pre-trained model is possible but in our experiments it didn’t perform as well compared to starting from a fine-tuned model.</p>

<h3 id="32-training-procedure"><strong>3.2 Training Procedure</strong></h3>

<p>Our goal is to preserve the model performance, so we opted to use knowledge distillation from the original model in the finetuning process to help the model imitate the behavior and learn their representation.</p>

<p>The loss function is the sum of three terms:</p>

<ol>
  <li>The logits loss with knowledge distillation using (Hinton et al., 2015) formulation which is a weighted average of two objectives. The first objective is the original cross entropy with correct labels. The second objective is the Kullback–Leibler divergence with the soft targets from the teacher. For the soft targets in the second objective we use the same high temperature in the softmax of the teacher and student model.</li>
  <li>Mean square error between the decoupled model final layer hidden representation with the original model final layer hidden representation.</li>
  <li>Mean square error between the decoupled model final layer self-attention representation with the  original model final layer self-attention representation.</li>
</ol>

<p>This training procedure inspired by TinyBERT except that we only apply the feature derived losses (hidden representation and self-attention) to the last layer only and we scale it by 0.5</p>

<h3 id="33-experiments-and-results"><strong>3.3 Experiments and Results</strong></h3>

<p>One of the big questions we were trying to answer is, How much cross-attention (between the inputs) is really needed? We evaluated our approach on a diverse set of datasets. We used SQUAD 2.0 (Machine Reading Comprehension), QQP and MRPC (paraphrasing identification) and MNLI (natural language Inference). We used ELECTRA-Base for all the datasets, except for SQUAD 2.0 we used ROaD-Base which is the current state of the art model for SQUAD 2.0 and is based on ELECTRA architecture as well.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/dc3.png" alt="desktopview" /></p>

<p>This figure shows the performance starting from the baseline (without decoupling, 0-12) to decoupling with 11 layers in the Input-Component part, and 1 in the Cross-Component part. We observed that tasks with a big dataset have similar behavior with noticeable drop when moving from 5 layers input-component - 7 layers cross–component to 6 layers input-component - 6 layers cross-component and another big drop when the number of cross-component layers becomes less than 3. While MRPC had similar characteristics the drop of performance was significant overall.</p>

<h2 id="4-compression"><strong>4. Compression</strong></h2>

<p>In the decoupled transformer we need to cache the representation produced by the decoupled transformer for one of the inputs, which could be a significant amount of storage. In case of question answering over Wikipedia where we cache the passage representation, this could be 6.8TB assuming (32 million passage, 150 token per passage, 768 token dimension for base BERT model, float16).</p>

<p>6.8TB could be a significant amount of storage, therefore we introduced a compression layer at the end of the Input-Component model and decompression layer at the start of the Cross-Component model, similar to autoencoders. The additional layer is a linear projection layer that projects the input to another (smaller or larger) dimension.</p>

<h3 id="41-training-procedure"><strong>4.1. Training Procedure</strong></h3>

<p>We start from a regular fine-tuned model, decoupe it as in the previous experiment and add the compression/decompression layers. We perform the finetuning in 2 phases. In the first phase we optimize the compression/decompression layer independent of the rest of the model, which means the cross-component part receives the representation without modification (without compression/decompression). In the second phase, we optimize everything jointly, which means the cross-component part receives the decompressed representation. The intuition behind this approach is that the model is already initialized (pre-trained and fine-tuned) while the compression/decompression layers aren’t so we first need to train those layers independent from the model and then use the decompressed representation to tune the coupled part of the model to understand the slightly different representation.</p>

<h3 id="42-experiments-and-results"><strong>4.2. Experiments and Results</strong></h3>

<p>We applied the compression approach on a 5 Input / 7 Cross MRC model on SQUAD 2.0 dataset to investigate how much impact does the different levels of compression have on the model performance.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/dc4.png" alt="desktopview" /></p>

<p>We observed the performance degradation is minimal for 2x, 3x and 4x compression and then it started to accelerate.</p>

<p>At 4x compression the required storage for Question answering over Wikipedia with the previous assumptions is 1.7TB which could be reduced 850GB if we used INT8</p>

<h2 id="5-final-model"><strong>5. Final Model</strong></h2>

<p>Our final decoupled model named: Decoupled ROaD-Base model with 4x compression is not just very efficient but perform better than other state-of-the-art models (e.g. DeBERTa, ELECTRA)</p>



      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
<!--        -->

        <!-- tags -->
<!--        -->

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          <div class="license-wrapper">
<!--          -->
<!--            -->
<!--            This post is licensed under -->
<!--            <a href="https://creativecommons.org/licenses/by/4.0/">-->
<!--              CC BY 4.0-->
<!--            </a>-->
<!--             by the author.-->
<!--          -->
          </div>

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hfadeel.github.io//posts/dc/" data-toggle="tooltip" data-placement="top"
          title="LinkedIn" target="_blank" rel="noopener" aria-label="LinkedIn">
          <i class="fa-fw fab fa-linkedin"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')"
        data-toggle="tooltip" data-placement="top"
        title="Copy link">
    </i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->





<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  <!---->

<!---->

<!---->

<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->
<!--  -->
<!--    -->
<!--    -->
<!--  -->
<!---->

<!---->

<!---->

<!---->
<!--  -->
<!---->
<!--  -->
<!---->
<!--  -->
<!---->
<!--  -->
<!---->
<!--  -->
<!---->


  

  

















  
  </div> <!-- .access -->

  
    <!-- BS-toc.js will be loaded at medium priority -->
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
    <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
      <nav id="toc" data-toggle="toc"></nav>
    </div>
  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">




    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center">
    <div class="footer-left">
      <p class="mb-0">
        © 2021
        <a href="">Haytham ElFadeel</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://hfadeel.github.io/{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup -->
  <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>





<script defer src="/assets/js/dist/post.min.js"></script>



<!-- commons -->

<script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>




  </body>

</html>

