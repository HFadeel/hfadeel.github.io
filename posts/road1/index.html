<!DOCTYPE html>







<html lang="en" 
  
    mode="light"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    <!-- i18n for `_javascript/utils/timeago.js` -->
    <meta name="day-prompt" content="days ago">
    <meta name="hour-prompt" content="hours ago">
    <meta name="minute-prompt" content="minutes ago">
    <meta name="justnow-prompt" content="just now">

    

    

  

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="ROaD-Electra Robustly Optimized and Distilled Electra 1" />
<meta name="author" content="Haytham ElFadeel" />
<meta property="og:locale" content="en" />
<meta name="description" content="Build state-of-the-art Transformer models for QnA, NLI and NLU." />
<meta property="og:description" content="Build state-of-the-art Transformer models for QnA, NLI and NLU." />
<link rel="canonical" href="https://hfadeel.github.io//posts/road1/" />
<meta property="og:url" content="https://hfadeel.github.io//posts/road1/" />
<meta property="og:site_name" content="Haytham ElFadeel" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-10T19:32:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ROaD-Electra Robustly Optimized and Distilled Electra 1" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://hfadeel.github.io//posts/road1/"},"@type":"BlogPosting","url":"https://hfadeel.github.io//posts/road1/","headline":"ROaD-Electra Robustly Optimized and Distilled Electra 1","dateModified":"2020-10-10T19:32:00-07:00","datePublished":"2020-10-10T19:32:00-07:00","author":{"@type":"Person","name":"Haytham ElFadeel"},"description":"Build state-of-the-art Transformer models for QnA, NLI and NLU.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <title>ROaD-Electra Robustly Optimized and Distilled Electra 1 | Haytham ElFadeel
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Haytham ElFadeel">
<meta name="application-name" content="Haytham ElFadeel">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
<!--  -->

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">


  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">



  <!-- Manific Popup -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">



  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end" lang="en">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Haytham ElFadeel</a>
    </div>
    <div class="site-subtitle font-italic"></div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://www.linkedin.com/in/hfadeel/" aria-label="linkedin"
        
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['hfadeelm','gmail.com'].join('@')" aria-label="email"
        
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<!--<div id="topbar-wrapper" class="row justify-content-center topbar-down">-->
<!--  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">-->
<!--    <span id="breadcrumb">-->

<!--    -->

<!--    -->

<!--      -->

<!--        -->
<!--          <span>-->
<!--            <a href="/">-->
<!--              Home-->
<!--            </a>-->
<!--          </span>-->

<!--        -->

<!--      -->

<!--        -->

<!--      -->

<!--        -->

<!--          -->
<!--            <span>ROaD-Electra Robustly Optimized and Distilled Electra 1</span>-->
<!--          -->

<!--        -->

<!--      -->

<!--    -->

<!--    </span>&lt;!&ndash; endof #breadcrumb &ndash;&gt;-->

<!--    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>-->

<!--    <div id="topbar-title">-->
<!--      -->
<!--Post-->
<!--      -->
<!--    </div>-->

<!--    <i id="search-trigger" class="fas fa-search fa-fw"></i>-->
<!--    <span id="search-wrapper" class="align-items-center">-->
<!--      <i class="fas fa-search fa-fw"></i>-->
<!--      <input class="form-control" id="search-input" type="search"-->
<!--        aria-label="search" autocomplete="off" placeholder="Search...">-->
<!--      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>-->
<!--    </span>-->
<!--    <span id="search-cancel" >Cancel</span>-->
<!--  </div>-->

<!--</div>-->


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->




<!-- images -->



  <!-- add CDN prefix if it exists -->

  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

  

  <!-- add image placehoder to prevent layout reflow -->

  

  

  
    
      
      
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  

  



<!-- Add lang-badge for code snippets -->




<!-- return -->





<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>ROaD-Electra Robustly Optimized and Distilled Electra 1</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Haytham ElFadeel
          </span>
          
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->






  on

<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sat, Oct 10, 2020,  7:32 PM -0700"
  >Oct 10, 2020<i class="unloaded">2020-10-10T19:32:00-07:00</i>
</span>

        </div>

        <div>
          <!-- lastmod -->
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<!--<span class="readtime" data-toggle="tooltip" data-placement="bottom"-->
<!--  title="1719 words">-->
<!--9 min-->
<!---->
<!--     read-->
<!---->
<!--</span>-->


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <p><sup>Build state-of-the-art Transformer models for QnA, NLI and NLU.</sup></p>

<h2 id="tldr"><strong>TL;DR</strong></h2>
<ul>
  <li>We present a new multi-task pre training method that improves the transformer performance, generalization, and robustness.</li>
  <li>We present a new variation of knowledge distillation that improves the teacher signal.</li>
  <li>A new state-of-the-art results in SQUAD 2.0 MNLI, and QQP. These improvements come without any increase in inference compute. This model has less weight than ELECTRA.</li>
</ul>

<h2 id="1-introduction"><strong>1. Introduction</strong></h2>

<p>The goal of this project was to push the state-of-the-art performance of transformer models (focusing on Question Answering and NLI). while simultaneously trying to address some of the transformer models issues such as: <span style="text-decoration:underline;">Robustness</span> (small paraphrasing leads to loss of accuracy) and <span style="text-decoration:underline;">Generalization</span> (model trained on one dataset, performs poorly on another dataset of the same task), <span style="text-decoration:underline;">Answerability</span> (models tend to answer even if there is no answer in the context). The goal is to do this without incurring any increase in inference computation.</p>

<p>This work does not target new pre-training objectives or methods, such as RoBERTa and ELECTRA; instead it builds on top of ELECTRA and explores other multi-task pretraining and knowledge distillation.</p>

<h2 id="2-baseline"><strong>2. Baseline</strong></h2>

<p>We selected ELECTRA (Clark et al., 2019) as our starting (‘baseline’) model since it represents the current state-of-the-art. That being said, our implementation differs in one aspect, ELECTRA question-answering module predicts the answer start and end positions jointly and has an ‘answerability’ classifier. We use a simplified question-answering module which independently predicts the answer start and end positions. We train the model to predict position ‘0’ (the CLS token) if the question is not answerable, which is similar to the original BERT model. Our experiment shows that this simplification impact on performance is marginal to non-existence, yet it reduces the number of parameters by more than 4 million.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road1.png" alt="desktopview" /></p>

<h2 id="3-multi-task-pretraining"><strong>3. Multi-task pretraining</strong></h2>

<p>Transformer models are trained in a self-supervised manner to predict a masked in a context, predict the next sentence or to distinguish “real” input tokens vs “fake” input tokens generated by another neural network in the case of ELECTRA.</p>

<p>Those objectives allow the models to learn about language structure and semantic, but its lack aspects of human language learning such as: humans learn to use language by listening and performing multiple tasks (e.g. expressing emotion, making statements, asking and answering, etc.) and they apply the knowledge learned from task to another to help learn new task. Also, language doesn’t exist on it’s own, language exists in a physical world that contains physical objects, interactions, general and common-sense knowledge, and a variety of sensory input e.g. vision, voice, etc. - but this topic for another day.</p>

<p>Multi-task pretraining is not a new idea, it has been proposed a few times (Caruana, 1997; Zhang and Yang, 2017; Liu et al, 2019). The goal of our MT-Pretraining is to teach the model a diverse set of realistic tasks to help it better understand language and generalize better.</p>

<h2 id="31-our-approach"><strong>3.1 Our Approach</strong></h2>

<p>Our model architecture is identical to ELECTRA. The encoding layers are shared across all tasks. Each task (a task could span multiple datasets) has its own output heads.</p>

<h2 id="32-tasks"><strong>3.2 Tasks</strong></h2>

<p>We selected 9 diverse publicly available datasets and categorized them into 6 tasks (model heads).</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2.png" alt="desktopview" /></p>

<h2 id="33-training-procedure"><strong>3.3 Training Procedure</strong></h2>

<p>To train out MT-Pretraining model, we start from a pre-trained ELECTRA-Large, we perform the multi-task pretraining then fine tune the model for a specific task.</p>

<p>In the multitask pre-training stage, we use Adam optimizer to update the parameters of our model (i.e. parameters of all shared layers and task-specific layers). In every training step, we perform multiple passes (forward, compute loss and gradian accumulate) using randomly selected dataset. Which means, we average the gradient across tasks.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road3.png" alt="desktopview" /></p>

<h2 id="34-similarity-to-mt-dnn"><strong>3.4 Similarity to MT-DNN</strong></h2>

<p>MT-DNN combines BERT with second MTL pretraining on supervised tasks to achieve improved performance on several NLU tasks. ROaD approach to MTL pretraining is similar to MT-DNN but it differs in two major ways: (1) We average the gradient across several tasks (randomly selected tasks). MT-DNN performs a step based on the gradient of a single task.  (2) We use the same prediction head for each task across datasets, whereas MT-DNN uses a separate head per dataset. For example, MRC tasks such as TriviaQA, NQ and QuAC share the same prediction head.</p>

<p>We experimented with the MT-DNN approach and we could not improve upon ELECTRA-Large, but we can re-produce their improvements on the original BERT.</p>

<h2 id="35-experiment-and-results"><strong>3.5 Experiment and Results</strong></h2>

<p>We compare ROaD MTL pretraining with MT-DNN (using our selection of datasets). We trained the model for 20,000 steps with a batch size of 512. We used a learning rate of 3e-4, layer-wise learning rate multiplayer of 0.75, 500 warm-up steps and linearly decreased the linear rate afterward. After the multi-task pretraining we fine-tuned the model on the target dataset.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road4.png" alt="desktopview" /></p>

<p>ROaD MTL consistently outperforms the baseline model and MT-DNN in all tasks and models. While we were able to reproduce MT-DNN improvements on the original BERT. MT-DNN didn’t improve upon ALBERT and degraded the ELECTRA performance. Our hypocrisies are the following:</p>

<p>The original BERT that MT-DNN used, was trained for significantly a smaller number of steps, on smaller training data using only 6% of the FLOPs used to train ELECTRA-Large and RoBERTa-LARGE.</p>

<p>Improving upon extremely optimized and tuned models (e.g. ELECTRA-Large) that already found a good local-minima is very difficult, therefore we hypothesis that moving such model in the direction of a single task is insufficient to improve model overall performance, therefore averaging the gradient from different tasks prevent the model from wondering in the wrong direction (a direction that is beneficial for a single task).</p>

<h3 id="generalization"><strong>Generalization</strong></h3>

<p>Improving generalization and robustness are an important theses of multi-task pretraining. To test that evaluate models on NewsQA. using a model that was fine-tuned on SQUAD 2.0 dataset. Which is OOD question-answering dataset that the model never seen or trained on. Here are the results:</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road5.png" alt="desktopview" /></p>

<h3 id="knowledge-distillation-in-multi-task-pretraining"><strong>Knowledge Distillation in Multi-task Pretraining</strong></h3>

<p>We also investigated the benefits of KD during Multi-task pretraining. We use logit (Hinton et al., 2015) KD. The procedure works like this: Fist, finetune teachers on each training dataset. Then, use the teacher to generate soft targets for each dataset. Finally, we use the generated soft targets with existing hard targets to pretrain a ROaD MTL model. During the MTL pre-training we define a loss function that is a weighted average of two objectives. The first objective is the original cross entropy (CE) with ground truth “hard targets”. The second objective is the Kullback–Leibler (KL) divergence with “soft targets” targets from the teacher’s predictions. For the second KL objective, we use softmax with high temperature T &gt; 3 to generate a softer probability distribution over classes. We set the same temperature in the teacher and student model.</p>

<p>Using KD during the MTL pre-training step with regular finetuning improves the model performance even further since it provides a richer pre-training singal.  In particular, on the out-of-domain and out-of-distribution NewsQA dataset (model used on this dataset was fine-tuned on SQUAD 2.0). Which demonstrates that models trained with KD during MTL pretraining were able to learn better universal language representation. In addition, the MNLI-M 91.4% and QQP 92.6% results are new best published accuracies for a single model.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road6.png" alt="desktopview" /></p>

<h2 id="4-knowledge-distillation"><strong>4. Knowledge Distillation</strong></h2>

<p>Knowledge Distillation is the process of transferring knowledge from a model to another model, usually a smaller one.</p>

<p>Supervised Machine learning relies on labels. Yet these labels provide limited signals. For example, in image recognition tasks, the labels are one-hot vectors with the entire probability assigned to the correct label; those labels do not provide any signal about the incorrect labels. For example, An image of a cow is relatively similar to an image of a bull, but many times less similar to an image of a chair. This similar structure is very valuable and could be acquired from a teacher model.</p>

<p>The training objective is to maximize the average log probability of the correct answer, but a side-effect of the learning process is that the model assigns probabilities to all classes (correct and incorrect). The relative probabilities of incorrect answers provide a similarity structure that could be used to improve the training signal of a student model.</p>

<p>For this work, we’re using Hinton et al., 2015 knowledge distillation formulation, which works at the logits level. We’re distilling from an ensemble of three same size MT-Pretrain or ELECTRA models.</p>

<p>While KD provides an increase in performance. One of the known limitations of knowledge distillation is that students are limited by the teacher’s performance.</p>

<p>We experimented with an approach called Teacher Annealing, in which the student early on tries to imitate the teacher then toward the end the student mostly relies on the gold-standard labels so it can learn to surpass its teachers. While this approach seems to be very promising and worth investigating further, we weren’t able to improve upon the typical KD after several experiments.</p>

<p>We also experimented with iterative KD aka born-again-networks (Furlanello et al., 2018) but we weren’t able to surpass the typical KD.</p>

<h2 id="41-kd-with-improved-signal"><strong>4.1 KD with improved signal</strong></h2>

<p>After those experiments, we turned our attention to the core question, how to improve the teacher signal. After several experiments we arrived at Weighted KD, which scales the logits according to the signal quality (correctness). Here is how to procedure work:</p>

<p>Given M models, Input (example input features, target (ground truth targets), and W scaling factor (hyperparameter W &lt; 1, typically 0.75). We scale the logits of the incorrect teacher by W and distribute the remaining weight to the correct teachers. The final weighting is computed such that to preserve the logits average value range. This way the student can focus on the information from the correct teachers. Here is the procedure:</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road7.png" alt="desktopview" /></p>

<h2 id="42-experiment-and-results"><strong>4.2 Experiment and Results</strong></h2>

<p>We compared the different knowledge distillation approaches. Here are the results:</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road8.png" alt="desktopview" /></p>

<h2 id="5-the-final-model"><strong>5. The Final Model</strong></h2>

<p>The final model is called Robustly Optimized and Distilled ELECTRA (ROaD-ELECTRA). Is trained using the presented Multi task pre-training, finetune on SQUAD (no data augmentation) with weighted knowledge distillation from ensemble of three same size models. Here are the results on SQUAD 2.0 dev and out of domain dataset NewsQA (a dataset the model has never seen):</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road9.png" alt="desktopview" /></p>

<p>91.6% F1 and 89.0 EM represent the highest score for a single model on SQUAD dev.</p>

<p>This model also generalizes better on out of domain datasets.</p>

<p>This model also outperforms some ensembled models as well.</p>


      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
<!--        -->

        <!-- tags -->
<!--        -->

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          <div class="license-wrapper">
<!--          -->
<!--            -->
<!--            This post is licensed under -->
<!--            <a href="https://creativecommons.org/licenses/by/4.0/">-->
<!--              CC BY 4.0-->
<!--            </a>-->
<!--             by the author.-->
<!--          -->
          </div>

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hfadeel.github.io//posts/road1/" data-toggle="tooltip" data-placement="top"
          title="LinkedIn" target="_blank" rel="noopener" aria-label="LinkedIn">
          <i class="fa-fw fab fa-linkedin"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')"
        data-toggle="tooltip" data-placement="top"
        title="Copy link">
    </i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->





<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  














  

  

















  
  </div> <!-- .access -->

  
    <!-- BS-toc.js will be loaded at medium priority -->
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
    <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
      <nav id="toc" data-toggle="toc"></nav>
    </div>
  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">




    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center">
    <div class="footer-left">
      <p class="mb-0">
        © 2021
        <a href="">Haytham ElFadeel</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://hfadeel.github.io/{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup -->
  <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>





<script defer src="/assets/js/dist/post.min.js"></script>



<!-- commons -->

<script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>




  </body>

</html>

