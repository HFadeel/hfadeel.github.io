<!DOCTYPE html>







<html lang="en" 
  
    mode="light"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    <!-- i18n for `_javascript/utils/timeago.js` -->
    <meta name="day-prompt" content="days ago">
    <meta name="hour-prompt" content="hours ago">
    <meta name="minute-prompt" content="minutes ago">
    <meta name="justnow-prompt" content="just now">

    

    

  

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="ROaD-Electra Robustly Optimized and Distilled Electra 2" />
<meta name="author" content="Haytham ElFadeel" />
<meta property="og:locale" content="en" />
<meta name="description" content="TL;DR We present a new state-of-the-art base transformer model for machine reading comprehension that has almost 5% higher F1 score compared to ELECTRA-Base and 2% higher F1 score than the previous state-of-the-art base model (DeBERTa). This Model also outperforms BERT-Large. We present more experiments on our new multi-task pre-training method and knowledge distillation which led to this new model." />
<meta property="og:description" content="TL;DR We present a new state-of-the-art base transformer model for machine reading comprehension that has almost 5% higher F1 score compared to ELECTRA-Base and 2% higher F1 score than the previous state-of-the-art base model (DeBERTa). This Model also outperforms BERT-Large. We present more experiments on our new multi-task pre-training method and knowledge distillation which led to this new model." />
<link rel="canonical" href="https://hfadeel.github.io//posts/road2/" />
<meta property="og:url" content="https://hfadeel.github.io//posts/road2/" />
<meta property="og:site_name" content="Haytham ElFadeel" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-25T19:32:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ROaD-Electra Robustly Optimized and Distilled Electra 2" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://hfadeel.github.io//posts/road2/"},"@type":"BlogPosting","url":"https://hfadeel.github.io//posts/road2/","headline":"ROaD-Electra Robustly Optimized and Distilled Electra 2","dateModified":"2021-08-16T23:56:30-07:00","datePublished":"2020-12-25T19:32:00-07:00","author":{"@type":"Person","name":"Haytham ElFadeel"},"description":"TL;DR We present a new state-of-the-art base transformer model for machine reading comprehension that has almost 5% higher F1 score compared to ELECTRA-Base and 2% higher F1 score than the previous state-of-the-art base model (DeBERTa). This Model also outperforms BERT-Large. We present more experiments on our new multi-task pre-training method and knowledge distillation which led to this new model.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <title>ROaD-Electra Robustly Optimized and Distilled Electra 2 | Haytham ElFadeel
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Haytham ElFadeel">
<meta name="application-name" content="Haytham ElFadeel">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
<!--  -->

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">


  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">



  <!-- Manific Popup -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">



  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end" lang="en">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Haytham ElFadeel</a>
    </div>
    <div class="site-subtitle font-italic"></div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://www.linkedin.com/in/hfadeel/" aria-label="linkedin"
        
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['hfadeelm','gmail.com'].join('@')" aria-label="email"
        
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<!--<div id="topbar-wrapper" class="row justify-content-center topbar-down">-->
<!--  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">-->
<!--    <span id="breadcrumb">-->

<!--    -->

<!--    -->

<!--      -->

<!--        -->
<!--          <span>-->
<!--            <a href="/">-->
<!--              Home-->
<!--            </a>-->
<!--          </span>-->

<!--        -->

<!--      -->

<!--        -->

<!--      -->

<!--        -->

<!--          -->
<!--            <span>ROaD-Electra Robustly Optimized and Distilled Electra 2</span>-->
<!--          -->

<!--        -->

<!--      -->

<!--    -->

<!--    </span>&lt;!&ndash; endof #breadcrumb &ndash;&gt;-->

<!--    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>-->

<!--    <div id="topbar-title">-->
<!--      -->
<!--Post-->
<!--      -->
<!--    </div>-->

<!--    <i id="search-trigger" class="fas fa-search fa-fw"></i>-->
<!--    <span id="search-wrapper" class="align-items-center">-->
<!--      <i class="fas fa-search fa-fw"></i>-->
<!--      <input class="form-control" id="search-input" type="search"-->
<!--        aria-label="search" autocomplete="off" placeholder="Search...">-->
<!--      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>-->
<!--    </span>-->
<!--    <span id="search-cancel" >Cancel</span>-->
<!--  </div>-->

<!--</div>-->


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->




<!-- images -->



  <!-- add CDN prefix if it exists -->

  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

  

  <!-- add image placehoder to prevent layout reflow -->

  

  

  
    
      
      
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  
    

    
    
    

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    
      
      

      

      

    

    

  

  



<!-- Add lang-badge for code snippets -->




<!-- return -->





<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>ROaD-Electra Robustly Optimized and Distilled Electra 2</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Haytham ElFadeel
          </span>
          
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->






  on

<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Fri, Dec 25, 2020,  7:32 PM -0700"
  >Dec 25, 2020<i class="unloaded">2020-12-25T19:32:00-07:00</i>
</span>

        </div>

        <div>
          <!-- lastmod -->
          
          <span>
            Updated
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->






<span class="timeago lastmod"
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Mon, Aug 16, 2021, 11:56 PM -0700"
  >Aug 16<i class="unloaded">2021-08-16T23:56:30-07:00</i>
</span>

          </span>
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<!--<span class="readtime" data-toggle="tooltip" data-placement="bottom"-->
<!--  title="1165 words">-->
<!--6 min-->
<!---->
<!--     read-->
<!---->
<!--</span>-->


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <h2 id="tldr"><strong>TL;DR</strong></h2>
<ul>
  <li>We present a new state-of-the-art base transformer model for machine reading comprehension that has almost 5% higher F1 score compared to ELECTRA-Base and 2% higher F1 score than the previous state-of-the-art base model (DeBERTa). This Model also outperforms BERT-Large.</li>
  <li>We present more experiments on our new multi-task pre-training method and knowledge distillation which led to this new model.</li>
</ul>

<h2 id="1-introduction"><strong>1. Introduction</strong></h2>

<p>In the previous installment of this series. I talked about Multi-task pre-training to improve generalization and set new state-of-the-art numbers in machine reading comprehension, NLI and QQP while also improving out-of-domain performance.
In this installment, I’m going to talk about building the best base model. As a reminder, the ROaD-Large model is based on ELECTRA architecture but with extra multi-task pre-training phase and weighted knowledge distillation.</p>

<h2 id="2-road-base-v10-baseline"><strong>2. ROaD-Base v1.0 (baseline):</strong></h2>

<p>Our first version of ROaD-Base was built by simply distilling from ROaD-Large (which was trained using <strong>our multi-task pre-training</strong> and fine-tuned with <strong>weighted KD</strong>) which gave us the best base model by far (4% in absolute improvements over ELECTRA-Base, 1.5% higher than the previous state-of-the-art DeBERTa Base).</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_1.png" alt="desktopview" /></p>

<h2 id="3-entering-road-base-v11"><strong>3. Entering ROaD-Base v1.1:</strong></h2>

<p>What is the best way to build base sized transformer models?! Traditionally, base models were built using the same techniques and procedures used to build the large models. But now with the proliferation of knowledge distillation we’re trying to change that. To answer this question, we did the following experiments:</p>

<ul>
  <li>Is it better to just distill from a large model(s) or perform multi-task pre-training as we did in ROaD-Large?</li>
  <li>In distillation: What would be a good teacher, a regular ELECTRA-Large, ROaD-Large or a group of Base models?</li>
  <li>In distillation: What form of distilling works best, logits vs hidden-state vs a combination?</li>
  <li>In multi-task pretraining: Should we average the gradient across different tasks (one of the core ideas in ROaD-Large) and if so how many tasks?</li>
</ul>

<h2 id="4-knowledge-distillation"><strong>4. Knowledge Distillation</strong></h2>

<h2 id="41-who-would-be-a-good-teacher"><strong>4.1 Who would be a good teacher?</strong></h2>

<ul>
  <li>
    <p><strong>Student:</strong> ELECTRA-Base</p>
  </li>
  <li>
    <p><strong>Possible teachers:</strong> ELECTRA-Large, ROaD-Large, ELECTRA-Base, an ensemble of models</p>
  </li>
  <li>
    <p><strong>Experiment Setting:</strong> We used logits based knowledge distillation (Hinton et al., 2015) with a loss function that is defined as a weighted average of two objectives. The first objective is the original cross entropy (CE) with ground truth “hard targets”. The second objective is the Kullback–Leibler (KL) divergence with “soft targets” targets from the teacher’s predictions. We performed several runs for each setting to identify the best hyper-parameter (mainly lambda and temperature) and reported the best results.</p>
  </li>
</ul>

<p><strong>Results:</strong></p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_2.png" alt="desktopview" /></p>

<p><strong>Observations and Take away:</strong></p>

<ul>
  <li>Distilling from a single larger model to smaller model seems to work better than self-distilling or ensemble self-distilling.</li>
  <li>Distilling from ROaD-Large is better than distilling from ELECTRA-Large which indicates that multitask pre-training makes the model a better teacher as well.</li>
  <li>Distilling from an ensemble of large models (ROaD-Large or ELECTRA-Large) is worse, my hypothesis is due to the increased gap in capacity between the teacher and student.</li>
  <li>ROaD-Large is the best teacher.</li>
</ul>

<h2 id="42-is-a-distilled-model-a-good-teacher"><strong>4.2 Is a distilled model a good teacher?</strong></h2>

<p>To answer this question we got the ROaD-Large (which was distilled from an ensemble of 3 ROaD-Large models) and ROaD-Large (without distillation) to see which one is a better teacher.
<strong>Results:</strong></p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_3.png" alt="desktopview" /></p>

<p><strong>Observations and Take away:</strong></p>

<ul>
  <li>Distilling from an undistilled model (a model that was not a student) is better than distilling from a distilled model (a model that was a student before).</li>
</ul>

<p><strong>Discussion:</strong></p>

<p>There are many possible hypotheses for this, such as: Distillation make the model a bad teacher (similar to label smoothing -<a href="https://l.workplace.com/l.php?u=https%3A%2F%2Farxiv.org%2Fpdf%2F1906.02629.pdf&amp;h=AT39Fj0P525iH0tuKlxSnrlOgsQOqxLU8hLEqbJ744WAs8W3RDCOlLZVNOSAJ3eAgejhdanCSqmN9Bl2s5NwRVehw0GUEe5IxBO9_S0mesOf96Ewxqddt1smBhyGEA85sYHoqPRs73QdbzwugTtCcQ"> https://arxiv.org/pdf/1906.02629.pdf</a><span style="text-decoration:underline;">)</span>, the increased gap in the model capacity and accuracy (but I don’t buy this one, because in previous experiments with ROaD-Large iterative distilling in which student then become a teacher for the next model never worked). This definitely require more investigation.</p>

<h2 id="43-what-form-of-distillation-works-the-best"><strong>4.3 What form of distillation works the best?</strong></h2>

<p>There are multiple forms of knowledge distillation, such as: using logits with KL loss, using the hidden state and/or attention of each layer with MSE loss (like TinyBert).</p>

<p>In the feature-based approach (hidden-state distillation) we project the hidden-state of the student model to match the size of the hidden-state of the teacher model. This projection is learned during the training. For the discrepancy in the number of layers we used the uniform mapping function as in the TinyBert paper (<a href="https://l.workplace.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F1909.10351&amp;h=AT39Fj0P525iH0tuKlxSnrlOgsQOqxLU8hLEqbJ744WAs8W3RDCOlLZVNOSAJ3eAgejhdanCSqmN9Bl2s5NwRVehw0GUEe5IxBO9_S0mesOf96Ewxqddt1smBhyGEA85sYHoqPRs73QdbzwugTtCcQ">https://arxiv.org/abs/1909.10351</a>), which means we map layer 24 -&gt; 12, 22 -&gt; 11 and so on. Based on experiments done as part of another project (Decoupling transformer - more in than in the near future) we decided to also try to distill the hidden-state of the last layer only (instead of every layer). Here are the results:</p>

<p><strong>Results:</strong></p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_4.png" alt="desktopview" /></p>

<p><strong>Observations and Take away:</strong></p>

<ul>
  <li>The addition of hidden-state matching losses is not sufficient on it’s own and doesn’t help that much.</li>
</ul>

<p><strong>Discussion:</strong></p>

<p>While the TinyBert paper produced great results, their use-case is different from ours. TinyBert built a model from scratch (a model that wasn’t initialized using pre-training) so they used this form of knowledge distillation during the pre-training to try to make the new model match the representation of the teacher. In our case the model already has decent representation from the pre-training and changing it to match the teacher may not be necessary!</p>

<h2 id="5-multitask-pre-training"><strong>5. Multitask Pre-training</strong></h2>

<h2 id="51-can-multitask-pre-training-help-build-a-better-model-also-how-many-tasks-to-average-the-gradient-from"><strong>5.1. Can multitask pre-training help build a better model? Also how many tasks to average the gradient from?</strong></h2>

<p><strong>Student:</strong> ELECTRA-Base</p>

<p>The typical approach of multitask learning is for each step we pick a task and perform forward, backward and update the model, and so on. In ROaD-Large work we discovered that while this approach worked for early models (e.g. BERT) it doesn’t improve upon the latest highly optimized model (e.g. ELECTRA). In ROaD-Large we averaged the gradient from 4 randomly selected tasks.</p>

<p>To answer if multitask pre-training help and how many tasks to average the gradient over, we come up with the following experiment: perform multitask pre-training with 1, 2, 4 and 8 passes (in each pass we pick a random task and compute the gradient) while trying to keep the batch size constant at 512. We performed 40,000 steps. For the finetuning we used logits knowledge distillation from ROaD-Large as well.</p>

<p><strong>Results:</strong></p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_5.png" alt="desktopview" /></p>

<p><strong>Observations and Take away:</strong></p>

<ul>
  <li>Multitask pre-training without gradient averaging does hinder the performance slightly compared to no multi-task pretraining.</li>
  <li>Multitask pre-training with gradient averaging, especially from 4 or 2 tasks, does improve performance.</li>
</ul>

<h2 id="6-final-model"><strong>6. Final Model</strong></h2>

<p>Putting everything together. We started from ELECTRA-Base (our baseline) we performed multitask pre-training for 40K steps while averaging the gradient from 4 tasks, then finetune with logits knowledge distillation from ROaD-Large (a model that was not a student).</p>

<p>The results, ROaD-Base v1.1 the best base sized Transformer model with about 5% higher F1 score in SQUAD 2.0 dev than the baseline. About 2% higher F1 and EM score in SQUAD 2.0 dev compared to the previous state-of-the-art DeBERTa. The model also outperforms BERT-Large.</p>

<p><strong>Results:</strong></p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_6.png" alt="desktopview" /></p>

<p>Outside of MRC the new model performs well. Here are the results on MNLI, QQP and Amazon. No knowledge distillation was performed, just a single fine-tuning experiment with the recommended hyper-parameter from ELECTRA paper.</p>

<p><img data-proofer-ignore data-src="/assets/img/blog/road2_7.png" alt="desktopview" /></p>


      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
<!--        -->

        <!-- tags -->
<!--        -->

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          <div class="license-wrapper">
<!--          -->
<!--            -->
<!--            This post is licensed under -->
<!--            <a href="https://creativecommons.org/licenses/by/4.0/">-->
<!--              CC BY 4.0-->
<!--            </a>-->
<!--             by the author.-->
<!--          -->
          </div>

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://hfadeel.github.io//posts/road2/" data-toggle="tooltip" data-placement="top"
          title="LinkedIn" target="_blank" rel="noopener" aria-label="LinkedIn">
          <i class="fa-fw fab fa-linkedin"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')"
        data-toggle="tooltip" data-placement="top"
        title="Copy link">
    </i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->





<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  














  

    <div id="access-lastmod" class="post">
      <span>Recent Update</span>
      <ul class="post-content pl-0 pb-1 ml-1 mt-2">

      
        
        
        
        <li><a href="/posts/gls/">Gaussian Label Smoothing</a></li>
      
        
        
        
        <li><a href="/posts/ai/">How academic division of AI, limits AI progress</a></li>
      
        
        
        
        <li><a href="/posts/understanding/">What is understanding - AI Prospective</a></li>
      
        
        
        
        <li><a href="/posts/count-zeros/">Fast way to count zeros</a></li>
      
        
        
        
        <li><a href="/posts/old_tricks/">New Models and Old Tricks</a></li>
      

      </ul>
    </div> <!-- #access-lastmod -->

  

  

















  
  </div> <!-- .access -->

  
    <!-- BS-toc.js will be loaded at medium priority -->
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
    <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
      <nav id="toc" data-toggle="toc"></nav>
    </div>
  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">




    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center">
    <div class="footer-left">
      <p class="mb-0">
        © 2021
        <a href="">Haytham ElFadeel</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://hfadeel.github.io/{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup -->
  <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>





<script defer src="/assets/js/dist/post.min.js"></script>



<!-- commons -->

<script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>




  </body>

</html>

